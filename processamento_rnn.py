# -*- coding: utf-8 -*-
"""processamento_rnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mB3HzzAlLFEupK7UIhCKw-FGbvQ8SyKv

"""


"""# Imports"""

import os
import pandas as pd
from rich import print
import torchaudio
torchaudio.set_audio_backend("sox_io")
import torch
import torch.nn as nn
from torch.utils.data                        import DataLoader
from torch.utils.data import Dataset
import torch.optim                           as optim
from torch.nn.utils.rnn import pad_sequence
from pydub                                   import AudioSegment
import numpy                                 as np
import librosa
import matplotlib.pyplot                     as plt
import sys
import librosa
import librosa.display

"""# Data Set Path"""

path = "/extra/pbma/musicnet"
path_goncas = "/extra/pbma/musicnet"

"""# Output functions"""

def using_cuda():
    if torch.cuda.is_available():
        print("Using GPU for training")
    else:
        print("Using CPU for training")

DEFAULT_WIDTH  = 80
DEFAULT_RANGE  = 35
TITLE_COLOR    = "yellow"
DEFAULT_SYMBOL = '='

def print_header(title: str, args: dict = None, symbol: str = DEFAULT_SYMBOL, width: int = DEFAULT_WIDTH, param_range: int = DEFAULT_RANGE, color: str = TITLE_COLOR):
    """Prints a header for the specified section with a title."""
    print('\n')
    print(symbol * width)
    print(f"[{color}]{title.center(width)}[{color}]")
    print(symbol * width + "\n")

    # print aditional arguments
    if args is not None: print_arguments(args, param_range)

def print_arguments(args: dict, param_range: int = DEFAULT_RANGE):
    # print aditional arguments
    for key, value in args.items():
        print(f"> {key:<{param_range}}: {value}")

def print_small_header(title: str, symbol: str = DEFAULT_SYMBOL, width: int = DEFAULT_WIDTH, color: str = TITLE_COLOR):
    print(symbol * width)
    print(f"[{color}]{title.center(width)}[{color}]\n")

def print_separator(symbol: str = DEFAULT_SYMBOL, width: int = DEFAULT_WIDTH):
    print("\n" + symbol * width + "\n")

def print_update(s: str):
    print(f"{s}")

"""# Custom DataSet Class"""

class AudioDataset(Dataset):
    def __init__(self, data, target_length=16000):  # ~1 second at 16kHz
        self.data = data
        self.target_length = target_length

    def __getitem__(self, idx):
        item = self.data[idx]
        waveform = torch.tensor(item['audio'], dtype=torch.float32).squeeze()

        # Pad or truncate
        length = waveform.shape[0]
        if length < self.target_length:
            pad_len = self.target_length - length
            waveform = torch.nn.functional.pad(waveform, (0, pad_len))
        else:
            waveform = waveform[:self.target_length]

        label_vector = torch.zeros(88, dtype=torch.float32)
        for note in item['label']:
            if 0 <= note < 88:
                label_vector[note] = 1.0

        return waveform, label_vector

"""# Mel Spectogram"""

def compute_mel_spectrogram(tensor_audio, sr=22050, n_mels=128, fmax=8000):
    audio_np = tensor_audio.numpy().squeeze()
    mel = librosa.feature.melspectrogram(y=audio_np, sr=sr, n_mels=n_mels, fmax=fmax)
    mel_db = librosa.power_to_db(mel, ref=np.max)
    return mel_db

class MelSpectrogramDataset(Dataset):
    def __init__(self, data, n_notes=88, max_len=4000):  # max_len in time frames
        self.data = data
        self.n_notes = n_notes
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        mel = self.data[idx]['mel']
        label_df = self.data[idx]['label']

        mel_tensor = torch.tensor(mel, dtype=torch.float32).T  # [T, 128]

        # Pad or truncate in time dimension
        if mel_tensor.shape[0] > self.max_len:
            mel_tensor = mel_tensor[:self.max_len, :]
        else:
            pad_len = self.max_len - mel_tensor.shape[0]
            mel_tensor = torch.nn.functional.pad(mel_tensor, (0, 0, 0, pad_len))  # pad time dimension

        # Create multi-hot label vector
        label_vector = torch.zeros(self.n_notes)
        for note in label_df['note'].values:
            if 21 <= note <= 108:
                label_vector[note - 21] = 1.0

        return mel_tensor, label_vector

"""# RNN"""

class TranscriptionRNN(nn.Module):
    def __init__(self, n_notes):
        super(TranscriptionRNN, self).__init__()
        self.rnn = nn.LSTM(input_size=128, hidden_size=256, num_layers=2, batch_first=True, bidirectional=True)
        # Bidirectional LSTM with 2 layers
        # self.rnn_dropout = nn.Dropout(0.5)
        self.fc = nn.Sequential(
            nn.Linear(512, 512),  # <- corrected
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, n_notes)
        )

    def forward(self, x):
        x, _ = self.rnn(x)  # Pass through RNN layers
        # x = self.rnn_dropout(x)
        x = x[:, -1, :]  # Get the last time step output
        x = self.fc(x)  # Fully connected layers
        x = torch.sigmoid(x)  # Apply sigmoid activation to ensure output in [0, 1]
        return x

def load_wav_and_labels(train_audio_dir, test_audio_dir, train_label_dir, test_label_dir, load_fraction=0.30):
    data = {'train': [], 'test': []}

    def load_data(audio_dir, label_dir, split):
        wav_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]
        wav_files = wav_files[:int(len(wav_files) * load_fraction)]  # Only take the first half

        for filename in wav_files:
            wav_path = os.path.join(audio_dir, filename)
            label_path = os.path.join(label_dir, filename.replace('.wav', '.csv'))

            try:
                audio, sr = torchaudio.load(wav_path)
                label = pd.read_csv(label_path) if os.path.exists(label_path) else None
                data[split].append({
                    'filename': filename,
                    'audio': audio,
                    'sr': sr,
                    'label': label
                })
            except Exception as e:
                print(f"Error loading {filename}: {e}")

    load_data(train_audio_dir, train_label_dir, 'train')
    load_data(test_audio_dir, test_label_dir, 'test')

    return data

def train_test_split_data(file_paths, labels, test_size=0.2):
    # Since we already have train/test sets, we don't need to split again here
    return file_paths['train'], file_paths['test'], labels['train'], labels['test']

"""# Fetch Data"""

def fetch_data(musicnet_path="/extra/pbma/musicnet"):

    print_header("Finding Folder of Musicnet")

    # Path to the music_net data folder
    raw_data_file = musicnet_path

    #if os.path.exists(raw_data_file):
    #    print(f"Path found: {raw_data_file}")
    #    if not os.path.isdir(raw_data_file):
    #        print(f"Path is not a directory: {raw_data_file}")
    #        print("Please specify the correct path to the musicnet data folder.")
    #        sys.exit(1)
    #else:
    #    print(f"Path not found: {raw_data_file}")
    #    print("Please specify the correct path to the musicnet data folder.")
    #    sys.exit(1)

    return raw_data_file

"""# Load Split"""

def load_split(data, musicnet_path="/extra/pbma/musicnet"):

    print_header("Loading all the data")

    # Define your specific paths
    train_audio_dir = os.path.join(musicnet_path, 'train_data')
    test_audio_dir = os.path.join(musicnet_path, 'test_data')
    train_label_dir = os.path.join(musicnet_path, 'train_labels')
    test_label_dir = os.path.join(musicnet_path, 'test_labels')
    # train_audio_dir = '/Users/denis/Desktop/IST/S2_24_25/PMBA/musicnet/train_data'
    # test_audio_dir = '/Users/denis/Desktop/IST/S2_24_25/PMBA/musicnet/test_data'
    # train_label_dir = '/Users/denis/Desktop/IST/S2_24_25/PMBA/musicnet/train_labels'
    # test_label_dir = '/Users/denis/Desktop/IST/S2_24_25/PMBA/musicnet/test_labels'

    # Call the function with these paths
    data = load_wav_and_labels(train_audio_dir, test_audio_dir, train_label_dir, test_label_dir)

    # Example to access loaded data for training
    train_data = data['train']
    train_data_audio = []
    train_data_labels = []

    # Loop through each entry in train_data and separate the audio and labels
    for entry in train_data:
        audio = entry['audio']  # This will be the tensor with audio data
        label = entry['label']  # This will be the DataFrame with the labels

        # Append the audio and label to their respective lists
        train_data_audio.append(audio)
        train_data_labels.append(label)
    print(f"Number of training samples: {len(train_data)}")
    print(train_data_audio[0])

    # Example to access loaded data for testing
    test_data = data['test']
    test_data_audio = []
    test_data_labels = []

    print(f"Number of testing samples: {len(test_data)}")
    # Loop through each entry in train_data and separate the audio and labels
    for entry in test_data:
        audio = entry['audio']  # This will be the tensor with audio data
        label = entry['label']  # This will be the DataFrame with the labels

        # Append the audio and label to their respective lists
        test_data_audio.append(audio)
        test_data_labels.append(label)

    print(test_data_audio[0])

    return train_data_audio, train_data_labels, test_data_audio, test_data_labels

"""# Spectogram"""

def prepare_data_and_compute_spectogram(train_data_audio, train_data_labels, test_data_audio, test_data_labels):

    print_header("Applying spectrogram to the data")

    # Lists to store results
    train_mels = []
    test_mels = []

    # Process train data
    for audio_tensor in train_data_audio:
        try:
            mel_spec = compute_mel_spectrogram(audio_tensor)
            train_mels.append(mel_spec)
        except Exception as e:
            print(f"Error processing train sample: {e}")

    # Process test data
    for audio_tensor in test_data_audio:
        try:
            mel_spec = compute_mel_spectrogram(audio_tensor)
            test_mels.append(mel_spec)
        except Exception as e:
            print(f"Error processing test sample: {e}")

    print_update("Plotting Spectrogram of the First 3 samples")

    for i, mel in enumerate(train_mels[:3]):  # Just plot first 3
        plt.figure(figsize=(10, 4))
        librosa.display.specshow(mel, sr=22050, x_axis='time', y_axis='mel', fmax=8000)
        plt.colorbar(format='%+2.0f dB')
        plt.title(f'Train Sample {i} - Mel Spectrogram')
        plt.tight_layout()
        plt.show()

    # Create a new dataset combining mel spectrograms with labels
    train_dataset = [{'mel': mel, 'label': label} for mel, label in zip(train_mels, train_data_labels)]
    test_dataset  = [{'mel': mel, 'label': label} for mel, label in zip(test_mels, test_data_labels)]

    return train_dataset, test_dataset

"""# Data With Audio Only (No spectogram)"""

def prepare_audio_data_without_spectrogram(train_data_audio, train_data_labels, test_data_audio, test_data_labels):
    print_header("Preparing audio data without spectrogram")

    # Create a new dataset combining audio with labels
    train_dataset = [{'audio': audio, 'label': label} for audio, label in zip(train_data_audio, train_data_labels)]
    test_dataset  = [{'audio': audio, 'label': label} for audio, label in zip(test_data_audio, test_data_labels)]

    return train_dataset, test_dataset

"""# RNN Training"""

def collate_fn(batch):
    audios, labels = zip(*batch)

    # Convert to 1D float tensors and squeeze if necessary
    audios = [a.float().squeeze() for a in audios]
    labels = [l.float() for l in labels]

    # Pad to max length in the batch
    audios_padded = pad_sequence(audios, batch_first=True)  # (B, max_len)
    audios_padded = audios_padded.unsqueeze(1)  # (B, 1, max_len) for CNNs/RNNs expecting 3D input

    labels = torch.stack(labels)

    return audios_padded, labels

def train_BLSTM(train_dataset, test_dataset):
    print_header("Initialize the model")

    train_dataset = MelSpectrogramDataset(train_dataset, n_notes=88)
    test_dataset = MelSpectrogramDataset(test_dataset, n_notes=88)

    #train_dataset = AudioDataset(train_dataset_raw)
    #test_dataset = AudioDataset(test_dataset_raw)


    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

    cuda_model = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Check for GPU
    using_cuda()
    model = TranscriptionRNN(n_notes=88).to(cuda_model)  # Adjust for the number of notes (88 for piano keys)
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.BCELoss()  # Binary Cross-Entropy for multi-label classification

    # Training Loop
    num_epochs = 20
    for epoch in range(num_epochs):
        model
        model.train()
        running_loss = 0
        for inputs, labels in train_loader:
            inputs = inputs.to(cuda_model)  # Ensure float
            labels = labels.to(cuda_model)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}")

    return train_loader, test_loader, model, cuda_model

"""# Model Evaluation"""

def evaluate_model(model, test_loader, device):

    print_header("Evaluate the model")

    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            predicted = outputs.round()  # Since this is multi-label, round to 0/1
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f'Accuracy on test set: {correct / total:.2f}%')

"""# "Main Function"
"""

#path1 = "/content/drive/My Drive/IST/musicnet"
path_goncalo = "/extra/pbma/musicnet"

if os.path.exists(path_goncalo):
  print("fixe")
else:
  print(":(")

musicnet_path = fetch_data(path_goncalo)
train_data_audio, train_data_labels, test_data_audio, test_data_labels = load_split(musicnet_path)

print_header("Finished Loading")

"""# Call and train CNN"""

train_dataset, test_dataset = prepare_data_and_compute_spectogram(train_data_audio, train_data_labels, test_data_audio, test_data_labels)

"""# Call and train RNN"""

#train_dataset, test_dataset = prepare_audio_data_without_spectrogram(train_data_audio, train_data_labels, test_data_audio, test_data_labels)
train_loader, test_loader, model, device = train_BLSTM(train_dataset, test_dataset)

"""# Evaluate Model"""

evaluate_model(model, test_loader, device)